{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = Path(\"data/justforfun_persian.pdf\")\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Couldn't find the PDF at {PDF_PATH.resolve()}\")\n",
    "loader = PyPDFium2Loader(str(PDF_PATH))\n",
    "pdf_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages (loaded Document objects): 204\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages (loaded Document objects):\", len(pdf_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تولد یͷ نرد، بخش دوم صفحۀ ١١\n",
      "و او مͬ خواست من را هم در این تجربه شریͷ کند. همچنین مͬ خواست من را به ریاضͬ\n",
      "علاقمند کند.\n",
      "پس من را روی زانو هایش مͬ نشاند و از من مͬ خواست تا برنامه هایی که با دقت روی کاغذ\n",
      "نوشته بود را برایش تایپ کنم. مͬ گفت خودش با کامپیوترها راحت نیست. نمͬ دانم آن محاسبات\n",
      "راجع به چه چیزی بودند و بعید مͬ دانم که آن موقع هیچ درکͬ از کاری که مͬ کردم هم داشته باشم\n",
      "ولͬ به هرحال آنجا بودم و به او کمͷ مͬ کردم. احتمالا کار از حالتͬ که او خودش به تنهایی\n",
      "برنامه ها را وارد مͬ کرد، خیلͬ بیشتر طول مͬ کشید. ولͬ کسͬ چه مͬ داند؟ من از همان کودکͬ به\n",
      "صفحه کلید عادت کرده بودم، چیزی که پدربزرگم هیچ وقت امͺان اش را نداشت. بعد از مدرسه\n",
      "یا هر موقع دیͽری که مادرم من را پیش پدربزرگم مͬ گذاشت، مشغول همین کار مͬ شدیم.\n",
      "بعد شروع کردم به خواندن راهنماهای کامپیوتر و وارد کردن برنامه های آماده شده. مثال ها\n",
      "شامل بازی های ساده ای بودند که خودتان مͬ توانستید آن ها را وارد کنید. اگر همه چیز را درست\n",
      "تایپ مͬ کردید، یͷ آقایی با گرافیͷ بد، روی صفحه راه مͬ رفت. بعد مͬ توانستید برنامه را عوض\n",
      "کنید تا آقای راه رونده، رنگش عوض شود. شما خودتان مͬ توانستید این کار را بͺنید.\n",
      "این بالاترین لذت بود.\n",
      "شروع کردم به نوشتن برنامه های خودم. اولین برنامه ای که نوشتم، اولین برنامه ای بود که هر\n",
      "کسͬ مͬ نویسد:\n",
      "10 PRINT \"HELLO\"\n",
      "20 GOTO 10\n",
      "این برنامه دقیقا همان کاری را مͬ کند که انتظار دارید بͺند. روی صفحه مͬ نویسد “سلام” و\n",
      "تا ابد به این کار ادامه مͬ دهد. یا حداقل تا وقتͬ که شما از شدت سر رفتن حوصله تان، برنامه را\n",
      "قطع کنید.\n",
      "اما این قدم اول است. بعضͬ ها همین جا متوقف مͬ شوند.برای آن ها این برنامه احمقانه ای\n",
      "است چون “چرا باید کسͬ علاقمند باشد به میلیون ها کلمه ‘سلام’ خیره شود؟” اما به هرحال این\n",
      "برنامه تقریبا همیشه اولین برنامه در راهنماهایی بود که آن روزها همراه کامپیوترهای شخصͬ داده\n",
      "مͬ شدند.\n",
      "نکته جادویی اینجا است که شما مͬ توانید این برنامه را تغییر دهید. خواهرم مͬ گوید که من\n",
      "یͷ تغییر ریشه ای در برنامه دادم تا نسخه دومͬ بسازم که به جای نوشتن “سلام”، روی صفحه\n",
      "بارها و بارها مͬ نوشت “سارا بهترین است.” در کل من برادر بزرگ تر مهربانͬ نبودم ولͬ این ژست\n",
      "برنامه نویس،ͬ تاثیر زیادی روی خواهرم گذاشت.\n",
      "من این جریان را یادم نیست. هر بار که یͷ برنامه مͬ نوشتم، آن را فراموش مͬ کردم و سراغ\n",
      "برنامه بعدی مͬ رفتم.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_index = 15\n",
    "if page_index >= len(pdf_docs):\n",
    "    raise IndexError(\n",
    "        f\"Document has only {len(pdf_docs)} pages; can't show index {page_index}.\"\n",
    "    )\n",
    "print(pdf_docs[page_index].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars: 166  |  Suspects: 32\n",
      "'ͬ'  U+036C  COMBINING LATIN SMALL LETTER R  cat=Mn  count=7751\n",
      "'ͺ'  U+037A  GREEK YPOGEGRAMMENI  cat=Lm  count=1595\n",
      "'ͷ'  U+0377  GREEK SMALL LETTER PAMPHYLIAN DIGAMMA  cat=Ll  count=1559\n",
      "'ͽ'  U+037D  GREEK SMALL REVERSED DOTTED LUNATE SIGMA SYMBOL  cat=Ll  count=915\n",
      "'“'  U+201C  LEFT DOUBLE QUOTATION MARK  cat=Pi  count=348\n",
      "'”'  U+201D  RIGHT DOUBLE QUOTATION MARK  cat=Pf  count=348\n",
      "'‐'  U+2010  HYPHEN  cat=Pd  count=236\n",
      "'\\u0379'  U+0379  ?  cat=Cn  count=14\n",
      "'&'  U+0026  AMPERSAND  cat=Po  count=10\n",
      "'@'  U+0040  COMMERCIAL AT  cat=Po  count=10\n",
      "'˼'  U+02FC  MODIFIER LETTER END LOW TONE  cat=Sk  count=7\n",
      "'*'  U+002A  ASTERISK  cat=Po  count=7\n",
      "'\\ue03b'  U+E03B  ?  cat=Co  count=6\n",
      "'>'  U+003E  GREATER-THAN SIGN  cat=Sm  count=6\n",
      "'\\ue049'  U+E049  ?  cat=Co  count=5\n",
      "'\\ue03a'  U+E03A  ?  cat=Co  count=5\n",
      "'\\ue039'  U+E039  ?  cat=Co  count=5\n",
      "'’'  U+2019  RIGHT SINGLE QUOTATION MARK  cat=Pf  count=4\n",
      "'ˀ'  U+02C0  MODIFIER LETTER GLOTTAL STOP  cat=Lm  count=4\n",
      "'ˁ'  U+02C1  MODIFIER LETTER REVERSED GLOTTAL STOP  cat=Lm  count=4\n",
      "'‘'  U+2018  LEFT SINGLE QUOTATION MARK  cat=Pi  count=3\n",
      "'\\ue03c'  U+E03C  ?  cat=Co  count=3\n",
      "'<'  U+003C  LESS-THAN SIGN  cat=Sm  count=2\n",
      "'̞'  U+031E  COMBINING DOWN TACK BELOW  cat=Mn  count=1\n",
      "'ͯ'  U+036F  COMBINING LATIN SMALL LETTER X  cat=Mn  count=1\n",
      "'='  U+003D  EQUALS SIGN  cat=Sm  count=1\n",
      "'|'  U+007C  VERTICAL LINE  cat=Sm  count=1\n",
      "'#'  U+0023  NUMBER SIGN  cat=Po  count=1\n",
      "'©'  U+00A9  COPYRIGHT SIGN  cat=So  count=1\n",
      "'͒'  U+0352  COMBINING FERMATA  cat=Mn  count=1\n",
      "'ʿ'  U+02BF  MODIFIER LETTER LEFT HALF RING  cat=Lm  count=1\n",
      "'ö'  U+00F6  LATIN SMALL LETTER O WITH DIAERESIS  cat=Ll  count=1\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def collect_char_stats(docs):\n",
    "    cnt = Counter()\n",
    "    for d in docs:\n",
    "        cnt.update(d.page_content)\n",
    "    return cnt\n",
    "\n",
    "\n",
    "stats = collect_char_stats(pdf_docs)\n",
    "\n",
    "\n",
    "def is_expected_char(ch: str) -> bool:\n",
    "    cp = ord(ch)\n",
    "    if \"0\" <= ch <= \"9\" or \"A\" <= ch <= \"Z\" or \"a\" <= ch <= \"z\":\n",
    "        return True\n",
    "    if ch in \" \\n\\r\\t.,;:!?()[]{}«»'\\\"/\\\\-–—…،٪%‌\":\n",
    "        return True\n",
    "    if 0x0600 <= cp <= 0x06FF:\n",
    "        return True\n",
    "    if 0x0750 <= cp <= 0x077F:\n",
    "        return True\n",
    "    if 0x08A0 <= cp <= 0x08FF:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "suspects = [\n",
    "    (c, n, ord(c), unicodedata.name(c, \"?\"), unicodedata.category(c))\n",
    "    for c, n in stats.most_common()\n",
    "    if not is_expected_char(c) and c.strip() != \"\"\n",
    "]\n",
    "print(f\"Unique chars: {len(stats)}  |  Suspects: {len(suspects)}\")\n",
    "for c, n, code, name, cat in suspects[:50]:\n",
    "    print(f\"{repr(c)}  U+{code:04X}  {name}  cat={cat}  count={n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "MAP_GLOTTAL_TO_ARABIC = True\n",
    "CHAR_FIXES = {\n",
    "    \"ي\": \"ی\",\n",
    "    \"ك\": \"ک\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ۀ\": \"ه\",\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"ٱ\": \"ا\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ﻻ\": \"لا\",\n",
    "    \"ͷ\": \"ک\",\n",
    "    \"ͅ\": \"ی\",\n",
    "    \"ͬ\": \"ی\",\n",
    "    \"ͽ\": \"،\",\n",
    "    \"“\": \"«\",\n",
    "    \"”\": \"»\",\n",
    "    \"‘\": \"'\",\n",
    "    \"’\": \"'\",\n",
    "    \"‐\": \"-\",\n",
    "    \"\\u0379\": \"\",\n",
    "    \"\\u02fc\": \"\",\n",
    "}\n",
    "if MAP_GLOTTAL_TO_ARABIC:\n",
    "    CHAR_FIXES.update(\n",
    "        {\n",
    "            \"\\u02bf\": \"ع\",\n",
    "            \"\\u02c1\": \"ع\",\n",
    "            \"\\u02c0\": \"ء\",\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    CHAR_FIXES.update(\n",
    "        {\n",
    "            \"\\u02bf\": \"\",\n",
    "            \"\\u02c1\": \"\",\n",
    "            \"\\u02c0\": \"\",\n",
    "        }\n",
    "    )\n",
    "WS_EQUIVS = {\n",
    "    \"\\u00a0\": \" \",\n",
    "    \"\\u202f\": \" \",\n",
    "    \"\\u2000\": \" \",\n",
    "    \"\\u2001\": \" \",\n",
    "    \"\\u2002\": \" \",\n",
    "    \"\\u2003\": \" \",\n",
    "    \"\\u2004\": \" \",\n",
    "    \"\\u2005\": \" \",\n",
    "    \"\\u2006\": \" \",\n",
    "    \"\\u2007\": \" \",\n",
    "    \"\\u2008\": \" \",\n",
    "    \"\\u2009\": \" \",\n",
    "    \"\\u200a\": \" \",\n",
    "    \"\\u2060\": \" \",\n",
    "    \"\\u00ad\": \"\",\n",
    "}\n",
    "FORMAT_CHARS = \"\".join(\n",
    "    [\n",
    "        \"\\u200e\",\n",
    "        \"\\u200f\",\n",
    "        \"\\u202a\",\n",
    "        \"\\u202b\",\n",
    "        \"\\u202d\",\n",
    "        \"\\u202e\",\n",
    "        \"\\u202c\",\n",
    "        \"\\u2066\",\n",
    "        \"\\u2067\",\n",
    "        \"\\u2068\",\n",
    "        \"\\u2069\",\n",
    "        \"\\ufeff\",\n",
    "    ]\n",
    ")\n",
    "PUA_BULLETS = {\"\\ue039\", \"\\ue03a\", \"\\ue03b\", \"\\ue03c\", \"\\ue049\"}\n",
    "PERSIAN_DIACRITICS = re.compile(r\"[\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]\")\n",
    "\n",
    "\n",
    "def normalize_pua(text: str) -> str:\n",
    "    text = re.sub(r\"(?m)^[ \\t]*[\" + \"\".join(PUA_BULLETS) + r\"][ \\t]*\", \"• \", text)\n",
    "    for pua in PUA_BULLETS:\n",
    "        text = text.replace(pua, \" - \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_unassigned(text: str) -> str:\n",
    "    return \"\".join(ch for ch in text if unicodedata.category(ch) != \"Cn\")\n",
    "\n",
    "\n",
    "def normalize_pdf_text_advanced(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    for k, v in WS_EQUIVS.items():\n",
    "        text = text.replace(k, v)\n",
    "    text = text.translate({ord(ch): None for ch in FORMAT_CHARS})\n",
    "    for bad, good in CHAR_FIXES.items():\n",
    "        text = text.replace(bad, good)\n",
    "    text = normalize_pua(text)\n",
    "    text = remove_unassigned(text)\n",
    "    text = re.sub(r\"(?m)^\\s*\\*\\s+\", \"• \", text)\n",
    "    text = re.sub(r\"(?<=\\s)&(?=\\s)\", \" و \", text)\n",
    "    text = PERSIAN_DIACRITICS.sub(\"\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+([،,:;!؟.])\", r\"\\1\", text)\n",
    "    text = re.sub(r\"([،,:;!؟.])([^\\s\\n])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"(?<![.!؟:؛\\-])\\n(?!\\n)\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تولد یک نرد، بخش دوم صفحه ١١ و او می خواست من را هم در این تجربه شریک کند. همچنین می خواست من را به ریاضی علاقمند کند.\n",
      "پس من را روی زانو هایش می نشاند و از من می خواست تا برنامه هایی که با دقت روی کاغذ نوشته بود را برایش تایپ کنم. می گفت خودش با کامپیوترها راحت نیست. نمی دانم آن محاسبات راجع به چه چیزی بودند و بعید می دانم که آن موقع هیچ درکی از کاری که می کردم هم داشته باشم ولی به هرحال آنجا بودم و به او کمک می کردم. احتمالا کار از حالتی که او خودش به تنهایی برنامه ها را وارد می کرد، خیلی بیشتر طول می کشید. ولی کسی چه می داند؟ من از همان کودکی به صفحه کلید عادت کرده بودم، چیزی که پدربزرگم هیچ وقت ام یان اش را نداشت. بعد از مدرسه یا هر موقع دی، ری که مادرم من را پیش پدربزرگم می گذاشت، مشغول همین کار می شدیم.\n",
      "بعد شروع کردم به خواندن راهنماهای کامپیوتر و وارد کردن برنامه های آماده شده. مثال ها شامل بازی های ساده ای بودند که خودتان می توانستید آن ها را وارد کنید. اگر همه چیز را درست تایپ می کردید، یک آقایی با گرافیک بد، روی صفحه راه می رفت. بعد می توانستید برنامه را عوض کنید تا آقای راه رونده، رنگش عوض شود. شما خودتان می توانستید این کار را ب ینید.\n",
      "این بالاترین لذت بود.\n",
      "شروع کردم به نوشتن برنامه های خودم. اولین برنامه ای که نوشتم، اولین برنامه ای بود که هر کسی می نویسد:\n",
      "10 PRINT \"HELL\n"
     ]
    }
   ],
   "source": [
    "clean_docs = [\n",
    "    Document(\n",
    "        page_content=normalize_pdf_text_advanced(d.page_content), metadata=d.metadata\n",
    "    )\n",
    "    for d in pdf_docs\n",
    "]\n",
    "\n",
    "print(clean_docs[min(15, len(clean_docs) - 1)].page_content[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://linuxbook.ir/all.html\",),\n",
    "    header_template={\"User-Agent\": \"Mozilla/5.0\"},\n",
    ")\n",
    "loader = WebBaseLoader(\"https://linuxbook.ir/all.html\")\n",
    "web_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "لینوکس و زندگی | لینوکس و زندگی\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "لینوکس و زندگی\n",
      "\n",
      "\n",
      "\n",
      "روی جلد\n",
      "درباره\n",
      "دانلود\n",
      "حمایت\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "لینوکس و زندگی\n",
      "\n",
      "\n",
      "فهرست \n",
      "لینوکس و زندگی. نوشته جادی www.jadi.netبرای دسترسی به آخرین نسخه کتاب در فرمت‌های مختلف و همچنین حمایت مالی از پروژه کتاب‌هایی برای گیک‌ها، به سایت www.jadi.ir و بخش حمایت مراجعه کنید.درباره این کتاب \n",
      "این کتاب سعی می کنه به خواننده ایده‌هایی درمورد زندگی و لینوکس بده. چرا زندگی؟ چون لینوکس یک فلسفه است و براومده از یک جامعه و کسی که می خواد توش موفق باشه با\n"
     ]
    }
   ],
   "source": [
    "print(web_docs[0].page_content[:512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "import time, random\n",
    "\n",
    "wiki_titles = [\n",
    "    \"ریچارد استالمن\",\n",
    "    \"لینوس توروالدز\",\n",
    "    \"لینوکس\",\n",
    "    \"پروژه گنو\",\n",
    "    \"نرم‌افزار آزاد\",\n",
    "    \"بنیاد نرم‌افزار آزاد\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_wiki_title(title: str, retries: int = 3):\n",
    "    last_err = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            loader = WikipediaLoader(query=title, lang=\"fa\", load_max_docs=1)\n",
    "            docs = loader.load()\n",
    "            if docs:\n",
    "                return docs[0]\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "        time.sleep(random.uniform(2.0, 5.0) + 0.5 * attempt)\n",
    "    try:\n",
    "        loader_en = WikipediaLoader(query=title, lang=\"en\", load_max_docs=1)\n",
    "        docs_en = loader_en.load()\n",
    "        if docs_en:\n",
    "            return docs_en[0]\n",
    "    except Exception as e:\n",
    "        last_err = e\n",
    "    raise RuntimeError(f\"Failed to load Wikipedia page for '{title}'\") from last_err\n",
    "\n",
    "\n",
    "wiki_docs = []\n",
    "for t in wiki_titles:\n",
    "    try:\n",
    "        doc = load_wiki_title(t, retries=3)\n",
    "        wiki_docs.append(doc)\n",
    "        time.sleep(random.uniform(1.0, 2.5))\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {t} failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wikipedia pages (loaded Document objects): 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of wikipedia pages (loaded Document objects):\", len(wiki_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████▏                             | 49/179 [00:00<00:00, 277.70it/s]/home/alaa/miniconda3/envs/quera/lib/python3.11/site-packages/langchain_community/document_loaders/html_bs.py:126: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(f, **self.bs_kwargs)\n",
      "100%|████████████████████████████████████████| 179/179 [00:01<00:00, 144.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, BSHTMLLoader\n",
    "from pathlib import Path\n",
    "\n",
    "HTML_DIR = Path(\"data/html\")\n",
    "if not HTML_DIR.exists():\n",
    "    HTML_DIR = Path(\"html\")\n",
    "if not HTML_DIR.exists():\n",
    "    raise FileNotFoundError(f\"HTML directory not found at: {HTML_DIR.resolve()}\")\n",
    "loader = DirectoryLoader(\n",
    "    str(HTML_DIR),\n",
    "    glob=\"**/*.html\",\n",
    "    loader_cls=BSHTMLLoader,\n",
    "    loader_kwargs={\"open_encoding\": \"utf-8\"},\n",
    "    show_progress=True,\n",
    "    use_multithreading=True,\n",
    ")\n",
    "html_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages (loaded Document objects): 179\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages (loaded Document objects):\", len(html_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of splitted documents: 2652\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_like = locals().get(\"clean_docs\") or locals().get(\"pdf_docs\")\n",
    "web_like = locals().get(\"web_docs\")\n",
    "wiki_like = locals().get(\"wiki_docs\")\n",
    "html_like = locals().get(\"html_docs\")\n",
    "all_docs = []\n",
    "for group in (pdf_like, web_like, wiki_like, html_like):\n",
    "    if group:\n",
    "        all_docs.extend(group)\n",
    "if not all_docs:\n",
    "    raise RuntimeError(\n",
    "        \"No documents found. Make sure you ran the previous loading cells successfully.\"\n",
    "    )\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"؛\", \"؟\", \".\", \"!\", \"،\", \" \", \"\"],\n",
    "    add_start_index=True,\n",
    "    length_function=len,\n",
    ")\n",
    "splitted_docs = [\n",
    "    d for d in splitter.split_documents(all_docs) if d.page_content.strip()\n",
    "]\n",
    "print(\"The number of splitted documents:\", len(splitted_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter COHERE_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\") or getpass.getpass(\"Enter COHERE_API_KEY: \")\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "    model=\"embed-multilingual-v3.0\",\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore ready. Items: 2652\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from pathlib import Path\n",
    "import time, random, math\n",
    "\n",
    "if not locals().get(\"splitted_docs\"):\n",
    "    raise RuntimeError(\"splitted_docs not found. Run the splitting cell first.\")\n",
    "PERSIST_DIR = Path(\"./collections/llm_corpus\")\n",
    "PERSIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    import chromadb\n",
    "\n",
    "    client = chromadb.PersistentClient(path=str(PERSIST_DIR))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"chromadb is required for persistence. Try: pip install chromadb\"\n",
    "    ) from e\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"llm_corpus\",\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "def maybe_persist(vs):\n",
    "    try:\n",
    "        cl = getattr(vs, \"_client\", None)\n",
    "        if cl and hasattr(cl, \"persist\"):\n",
    "            cl.persist()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def add_in_batches(\n",
    "    vs,\n",
    "    documents,\n",
    "    batch_size: int = 8,\n",
    "    base_sleep: float = 3.0,\n",
    "    polite_delay: tuple = (1.5, 3.5),\n",
    "    max_retries: int = 6,\n",
    "):\n",
    "    try:\n",
    "        from cohere.errors import TooManyRequestsError as Cohere429\n",
    "    except Exception:\n",
    "\n",
    "        class Cohere429(Exception): ...\n",
    "\n",
    "    total = len(documents)\n",
    "    for start in range(0, total, batch_size):\n",
    "        batch = documents[start : start + batch_size]\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                vs.add_documents(batch)\n",
    "                if ((start // batch_size) % 5) == 0:\n",
    "                    maybe_persist(vs)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                msg = str(e)\n",
    "                is_rate = (\n",
    "                    isinstance(e, Cohere429)\n",
    "                    or \"429\" in msg\n",
    "                    or \"TooManyRequests\" in msg\n",
    "                    or \"rate limit\" in msg.lower()\n",
    "                )\n",
    "                if is_rate and attempt < max_retries:\n",
    "                    sleep_s = base_sleep * (2 ** (attempt - 1)) + random.uniform(\n",
    "                        0.0, 1.2\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"[429] Backoff {sleep_s:.1f}s | batch {start//batch_size+1}/{math.ceil(total/batch_size)} | try {attempt}/{max_retries}\"\n",
    "                    )\n",
    "                    time.sleep(sleep_s)\n",
    "                    continue\n",
    "                raise\n",
    "        time.sleep(random.uniform(*polite_delay))\n",
    "    maybe_persist(vs)\n",
    "\n",
    "\n",
    "add_in_batches(vectorstore, splitted_docs, batch_size=8, base_sleep=3.0, max_retries=6)\n",
    "try:\n",
    "    count = vectorstore._collection.count()\n",
    "except Exception:\n",
    "    count = len(vectorstore.get()[\"ids\"])\n",
    "print(f\"Vectorstore ready. Items: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full‑Power RAG is ready. Use: res = ask('سوال شما چیست؟'); print(res['answer'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "if \"vectorstore\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"vectorstore not found. Please build embeddings & Chroma vector store first.\"\n",
    "    )\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 6, \"fetch_k\": 24, \"lambda_mult\": 0.5},\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs_for_prompt(docs, max_chars=1400):\n",
    "    chunks = []\n",
    "    for d in docs:\n",
    "        txt = d.page_content\n",
    "        if len(txt) > max_chars:\n",
    "            txt = txt[:max_chars]\n",
    "        src = (\n",
    "            d.metadata.get(\"source\")\n",
    "            or d.metadata.get(\"url\")\n",
    "            or d.metadata.get(\"file_path\")\n",
    "            or d.metadata.get(\"path\")\n",
    "        )\n",
    "        if src:\n",
    "            chunks.append(f\"[منبع: {src}]\\n{txt}\")\n",
    "        else:\n",
    "            chunks.append(txt)\n",
    "    return \"\\n\\n-----\\n\\n\".join(chunks)\n",
    "\n",
    "\n",
    "def top_unique_sources(docs, limit=5):\n",
    "    out, seen = [], set()\n",
    "    for d in docs:\n",
    "        src = (\n",
    "            d.metadata.get(\"source\")\n",
    "            or d.metadata.get(\"url\")\n",
    "            or d.metadata.get(\"file_path\")\n",
    "            or d.metadata.get(\"path\")\n",
    "        )\n",
    "        if not src or src in seen:\n",
    "            continue\n",
    "        seen.add(src)\n",
    "        out.append(\n",
    "            {\n",
    "                \"source\": src,\n",
    "                \"title\": d.metadata.get(\"title\"),\n",
    "                \"page\": d.metadata.get(\"page\") or d.metadata.get(\"page_number\"),\n",
    "                \"snippet\": d.page_content[:320],\n",
    "            }\n",
    "        )\n",
    "        if len(out) >= limit:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "SYSTEM = (\n",
    "    \"تو یک دستیار RAG فارسی هستی. با استفاده از متن‌های بازیابی‌شده، \"\n",
    "    \"پاسخی روشن، دقیق و کامل بنویس. اگر مفید است از بولت‌پوینت استفاده کن. \"\n",
    "    \"در پایان یک بخش «منابع» بیاور و ۳–۵ مرجع برتر را فهرست کن.\"\n",
    ")\n",
    "PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"سوال: {question}\\n\\n\"\n",
    "            \"متن‌های بازیابی‌شده:\\n{context}\\n\\n\"\n",
    "            \"پاسخ کامل و دقیق بده و در پایان «منابع» را لیست کن.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "llm = ChatCohere(model=\"command-r-08-2024\", temperature=0.2)\n",
    "core_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": RunnablePassthrough()\n",
    "        | retriever\n",
    "        | RunnableLambda(format_docs_for_prompt),\n",
    "    }\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def ask(question: str):\n",
    "    \"\"\"High-level RAG function.\n",
    "    Returns: {'answer': <rich text>, 'sources': [ {source,title,page,snippet}, ... ]}\n",
    "    \"\"\"\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    answer_text = core_chain.invoke(question)\n",
    "    return {\n",
    "        \"answer\": answer_text.strip(),\n",
    "        \"sources\": top_unique_sources(docs, limit=5),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Full‑Power RAG is ready. Use: res = ask('سوال شما چیست؟'); print(res['answer'])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547613/474562247.py:95: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بهترین زبان برنامه‌نویسی به عوامل مختلفی بستگی دارد، از جمله اهداف، زمینه کاری، و ترجیحات شخصی. با این حال، برخی زبان‌ها به دلیل قدرت، انعطاف‌پذیری، و کاربرد گسترده‌شان، به عنوان گزینه‌های برتر شناخته می‌شوند.\n",
      "\n",
      "- **Python:** پایتون یکی از محبوب‌ترین و پرکاربردترین زبان‌های برنامه‌نویسی است. ساده، خوانا، و دارای کتابخانه‌های گسترده‌ای است که آن را برای توسعه سریع نرم‌افزار مناسب می‌کند. پایتون در زمینه‌های مختلف، از هوش مصنوعی و یادگیری ماشین گرفته تا توسعه وب و تحلیل داده، کاربرد دارد.\n",
      "\n",
      "- **JavaScript:** جاوااسکریپت زبان اصلی وب است و برای توسعه تعاملی و پویا در مرورگرها استفاده می‌شود. با گسترش فناوری‌های وب، جاوااسکریپت به یک زبان قدرتمند و انعطاف‌پذیر تبدیل شده است که در توسعه سمت کلاینت و سمت سرور کاربرد دارد.\n",
      "\n",
      "- **Java:** جاوا زبان برنامه‌نویسی شیءگرا است که در زمینه‌های مختلف، از توسعه نرم‌افزار گرفته تا برنامه‌های موبایل و سیستم‌های سازمانی، کاربرد دارد. جاوا دارای کتابخانه‌های گسترده و جامعه توسعه‌دهندگان فعال است.\n",
      "\n",
      "- **C++:** سی‌پلاس‌پلاس زبان قدرتمندی است که برای توسعه نرم‌افزارها و سیستم‌های پیچیده استفاده می‌شود. سی‌پلاس‌پلاس در زمینه‌های مختلف، از بازی‌سازی گرفته تا برنامه‌های مالی و علمی، کاربرد دارد.\n",
      "\n",
      "- **Lisp (یا Scheme):** لیسپ زبانی است که در حوزه هوش مصنوعی شهرت بسیاری دارد و به دلیل قدرت و زیبایی‌اش مورد تحسین قرار می‌گیرد. لیسپ رویکردی منحصر به فرد به برنامه‌نویسی دارد و می‌تواند برای توسعه‌دهندگانی که به دنبال یادگیری و تجربه زبان‌های متفاوت هستند، گزینه جذابی باشد.\n",
      "\n",
      "نکته مهم این است که یادگیری برنامه‌نویسی فراتر از یادگیری دستور زبان یک زبان خاص است. یادگیری مفاهیم پایه‌ای برنامه‌نویسی، الگوریتم‌ها، و طراحی نرم‌افزار، اهمیت بیشتری دارد. با این حال، انتخاب زبان مناسب می‌تواند به سرعت یادگیری و پیشرفت شما کمک کند.\n",
      "\n",
      "منابع:\n",
      "- https://linuxbook.ir/all.html\n",
      "- data/html/stallman-computing.html\n",
      "- data/justforfun_persian.pdf\n",
      "[1] https://linuxbook.ir/all.html\n",
      "[2] data/html/stallman-computing.html\n",
      "[3] data/justforfun_persian.pdf\n"
     ]
    }
   ],
   "source": [
    "res = ask(\"بهترین زبان برنامه نویسی چیه؟\")\n",
    "print(res[\"answer\"])\n",
    "for i, s in enumerate(res[\"sources\"], 1):\n",
    "    print(f\"[{i}] {s['source']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
